HTML 5
HTML5 es un lenguaje markup (de hecho, las siglas de HTML significan Hyper Text Markup Language) usado para estructurar y presentar el contenido para la web. Es uno de los aspectos fundamentales para el funcionamiento de los sitios, pero no es el primero. Es de hecho la quinta revisión del estándar que fue creado en 1990. A fines del año pasado, la W3C la recomendó para transformarse en el estándar a ser usado en el desarrollo de proyectos venideros. Por así decirlo, qué es HTML5 está relacionado también con la entrada en decadencia del viejo estándar HTML 4, que se combinaba con otros lenguajes para producir los sitios que podemos ver hoy en día. Con HTML5, tenemos otras posibilidades para explotar usando menos recursos. Con HTML5, también entra en desuso el formato XHTML, dado que ya no sería necesaria su implementación.

HTML4 fue “declarado” el lenguaje oficial de la web en el año 2000, y tomó una década para comenzar a implementar el desarrollo de su nueva revisión. Esta nueva generación de HTML, se dice, pronto dominará el desarrollo en internet, pero introduce algunos cambios importantes que veremos dentro de algunas líneas. Por ende, para los desarrolladores de sitios web es importante conocer las ventajas de HTML5, considerando que algunas entidades se están moviendo en esta dirección. No solamente Google con su navegador Chrome, hace unos años, sino también Adobe hace unos meses, que removió el soporte de Flash para Android para dar paso a la llegada de HTML5.

Volviendo a qué es HTML5. Se trata de un sistema para formatear el layout de nuestras páginas, así como hacer algunos ajustes a su aspecto. Con HTML5, los navegadores como Firefox, Chrome, Explorer, Safari y más pueden saber cómo mostrar una determinada página web, saber dónde están los elementos, dónde poner las imágenes, dónde ubicar el texto. En este sentido, el HTML5 no se diferencia demasiado de su predecesor, un lenguaje del cual hablamos hace algunos meses en nuestra guía básica de HTML. La diferencia principal, sin embargo, es el nivel de sofisticación del código que podremos construir usando HTML5.

En términos de Markup, el HTML5 introduce algunos elementos que hacen que se aggiorne a los tiempos que corren. Así, muchas de las novedades están relacionadas con la forma de construir websites que se tiene en la actualidad. Una de las más importantes novedades está relacionada con la inserción de multimedia en los sitios web, que ahora contarán con etiquetas HTML especiales para poder ser incluidos. Por otro lado, algunos aspectos de diseño también son incluidos en el lenguaje, así como también algunos detalles de navegación. Veremos todo esto en algunas líneas.

CSS3
El nombre hojas de estilo en cascada viene del inglés Cascading Style Sheets, del que toma sus siglas. CSS es un lenguaje usado para definir la presentación de un documento estructurado escrito en HTML o XML (y por extensión en XHTML). El W3C(World Wide Web Consortium) es el encargado de formular la especificación de las hojas de estilo que servirán de estándar para los agentes de usuario o navegadores.
La idea que se encuentra detrás del desarrollo de CSS es separar la estructura de un documento de su presentación. 
La información de estilo puede ser adjuntada como un documento separado o en el mismo documento HTML. 

En este último caso podrían definirse estilos generales en la cabecera del documento o en cada etiqueta particular mediante el atributo "<style>".

El CSS sirve para definir la estética de un sitio web en un documento externo y eso mismo permite que modificando ese documento (la hoja CSS) podamos cambiar la estética entera de un sitio web, el mismo sitio web puede variar totalmente de estética cambiando solo la CSS, sin tocar para nada los documentos HTML o jsp o asp que lo componen. 

CSS es un lenguaje utilizado para dar estética a un documento HTML (colores, tamaños de las fuentes, tamaños de elemento, con css podemos establecer diferentes reglas que indicarán como debe presentarse un documento. Podemos indicar propiedades como el color, el tamaño de la letra, el tipo de letra, si es negrita, si es itálica, también se puede dar forma a otras cosas que no sean letras, como colores de fondo de una pagina, tamaños de un elemento (por ejemplo el alto y el ancho de una tabla. 

JavaScript
JavaScript (abreviado comúnmente JS) es un lenguaje de programación interpretado, dialecto del estándar ECMAScript. Se define como orientado a objetos,3? basado en prototipos, imperativo, débilmente tipado y dinámico.

Se utiliza principalmente en su forma del lado del cliente (client-side), implementado como parte de un navegador web permitiendo mejoras en la interfaz de usuario y páginas web dinámicas4? aunque existe una forma de JavaScript del lado del servidor (Server-side JavaScript o SSJS). Su uso en aplicaciones externas a la web, por ejemplo en documentos PDF, aplicaciones de escritorio (mayoritariamente widgets) es también significativo.

Desde el 2012, todos los navegadores modernos soportan completamente ECMAScript 5.1, una versión de javascript. Los navegadores más antiguos soportan por lo menos ECMAScript 3. La sexta edición se liberó en julio del 2015.5

JavaScript se diseñó con una sintaxis similar a C, aunque adopta nombres y convenciones del lenguaje de programación Java. Sin embargo, Java y JavaScript tienen semánticas y propósitos diferentes.

Todos los navegadores modernos interpretan el código JavaScript integrado en las páginas web. Para interactuar con una página web se provee al lenguaje JavaScript de una implementación del Document Object Model (DOM).

Tradicionalmente se venía utilizando en páginas web HTML para realizar operaciones y únicamente en el marco de la aplicación cliente, sin acceso a funciones del servidor. Actualmente es ampliamente utilizado para enviar y recibir información del servidor junto con ayuda de otras tecnologías como AJAX. JavaScript se interpreta en el agente de usuario al mismo tiempo que las sentencias van descargándose junto con el código HTML.

Progressive Web Apps

Progressive web apps (o aplicaciones web progresivas), es un término que se da a una nueva generación de aplicaciones que incrementan su funcionalidad, conforme las capacidades del dispositivo en el que se ejecutan, incrementan, de ahí la palabra progresiva. La siguiente parte del nombre web, hace referencia a que se construyen utilizando estándares de desarrollo web, algunos ya conocidos como HTML, CSS y javaScript; y una nueva generación de APIs de javaScript. La parte final app es porque las Progressive Web Apps se comportan como aplicaciones web nativas, pero usan tecnologías web.
En términos muy simplistas, son páginas web que se comportan como aplicaciones nativas. Es un oración muy simple, pero también muy profunda. Las apps nativas (iOs, Android por ejemplo), históricamente han tenido una serie de ventajas sobre las páginas web, ¿como cuáles? Almacenamiento local, ejecutarse offline, notificaciones push, performance, acceso a hardware, acceso al homescreen del dispositivo, entre otros.
Con el paso del tiempo, la brecha entre las web apps y las apps nativas, se ha ido reduciendo. 

Hace aproximadamente 6 años, HTML5 comenzó a tomar forma, como el concepto que constituía nuevas etiquetas, CSS3 y nuevas APIs de javaScript, cuyo objetivo era hacer las páginas web, más parecidas a las aplicaciones nativas. Ahí, conocimos a localStorage y webRTC, tuvimos acceso al hardware, desde el GPS, hasta la cámara, pasando claro por el micrófono y los altavoces; también nos presentaron nuevos eventos touch, drag&drop, web workers, web sockets que no eran hardware precisamente, pero que buscaban acercar la experiencia de las interfaces web, a la de las apps nativas.

Casi inmediatamente después, el proyecto Cordova trajo más vida al desarrollo web, a partir de Cordova nacieron Phonegap y Ionic, dos frameworks para desarrollo de "apps nativas", utilizando estándares web, ¿el problema? El problema es que las, nuevas, aplicaciones híbridas, no terminaban de cerrar la brecha.

Las aplicaciones híbridas ganaron en el terreno de la instalación, push notifications y acceso al hardware, pero, para muchos, perdieron en performance, UX y acceso offline. Esta generación de aplicaciones se sentía más como un parche, que como una solución. Hubieron proyectos que nacieron como apps híbridas, que eventualmente tuvieron que migrarse a apps nativas escritas en JAVA o en Swift.

Inteligencia Artificial

La inteligencia artificial (IA), es la inteligencia exhibida por máquinas. En ciencias de la computación, una máquina «inteligente» ideal es un agente racional flexible que percibe su entorno y lleva a cabo acciones que maximicen sus posibilidades de éxito en algún objetivo o tarea.1? Coloquialmente, el término inteligencia artificial se aplica cuando una máquina imita las funciones «cognitivas» que los humanos asocian con otras mentes humanas, como por ejemplo: «aprender» y «resolver problemas».2? A medida que las máquinas se vuelven cada vez más capaces, tecnología que alguna vez se pensó que requería de inteligencia se elimina de la definición. Por ejemplo, el reconocimiento óptico de caracteres ya no se percibe como un ejemplo de la «inteligencia artificial» habiéndose convertido en una tecnología común.3? Avances tecnológicos todavía clasificados como inteligencia artificial son los sistemas de conducción autónomos o los capaces de jugar al ajedrez o al Go.4?

Según Takeyas (2007) la IA es una rama de las ciencias computacionales encargada de estudiar modelos de cómputo capaces de realizar actividades propias de los seres humanos en base a dos de sus características primordiales: el razonamiento y la conducta.5?

En 1956, John McCarthy acuñó la expresión «inteligencia artificial», y la definió como «la ciencia e ingenio de hacer máquinas inteligentes, especialmente programas de cómputo inteligentes».
Representación del conocimiento. La representación es una cuestión clave a la hora de encontrar soluciones adecuadas a los problemas planteados. Si analizamos más detenidamente el término encontramos varias definiciones: según Barr y Feigenbaum, la representación del conocimiento es una combinación de estructuras de datos y procedimientos de interpretación que, si son utilizados correctamente por un programa, éste podrá exhibir una conducta inteligente; según Fariñas y Verdejo, la Inteligencia Artificial tiene como objetivo construir modelos computacionales que al ejecutarse resuelvan tareas con resultados similares a los obtenidos por una persona, por lo que el tema central de esta disciplina es el estudio del conocimiento y su manejo; y según Buchanan y Shortliffe, la Representación del Conocimiento en un programa de Inteligencia Artificial significa elegir una serie de convenciones para describir objetos, relaciones, y procesos en el mundo. Gran parte del esfuerzo realizado en la consecución de ordenadores inteligentes, según Rahael, ha sido caracterizado por el intento continuo de conseguir más y mejores estructuras de representación del conocimiento, junto con técnicas adecuadas para su manipulación, que permitiesen la resolución inteligente de algunos de los problemas ya planteados. Otra característica importante es la inclusión en los programas de Inteligencia artificial, aunque por separado, de los conocimientos y la unidad que controla y dirige la búsqueda de soluciones. Dada esta disposición, en estos programas la modificación, ampliación y actualización de los mismos es sencilla.

Machine Learning.

Machine Learning es una disciplina científica del ámbito de la Inteligencia Artificial que crea sistemas que aprenden automáticamente. Aprender en este contexto quiere decir identificar patrones complejos en millones de datos. La máquina que realmente aprende es un algoritmo que revisa los datos y es capaz de predecir comportamientos futuros. Automáticamente, también en este contexto, implica que estos sistemas se mejoran de forma autónoma con el tiempo, sin intervención humana. Veamos cómo funciona.
Una empresa de telefonía quiere saber qué clientes están en “peligro” de darse de baja de sus servicios para hacer acciones comerciales que eviten que se vayan a la competencia. ¿Cómo puede hacerlo? La empresa tiene muchos datos de los clientes, muchísimos: antigüedad, planes contratados, consumo diario, llamadas mensuales al servicio de atención al cliente, últimos cambios de planes contratados… pero seguramente los usa solo para facturar y para hacer estadísticas. ¿Qué más puede hacer con esos datos? Se pueden usar para predecir cuándo un cliente se va a dar de baja y gestionar la mejor acción que lo evite. En pocas palabras, con Machine Learning se puede pasar de ser reactivos a ser proactivos. Los datos históricos del conjunto de los clientes, debidamente organizados y tratados en bloque, generan una base de datos que se puede explotar para predecir futuros comportamientos, favorecer aquellos que mejoran los objetivos de negocio y evitar aquellos que son perjudiciales.

Esa cantidad ingente de datos son imposibles de analizar por una persona para sacar conclusiones y menos todavía para hacer predicciones. Los algoritmos en cambio sí pueden detectar patrones de comportamiento contando con las variables que le proporcionamos y descubrir cuáles son las que han llevado, en este caso, a darse de baja como cliente. La siguiente imagen es un ejemplo de una predicción simplificada basada en datos de una compañía de telefonía ficticia, pero usando una herramienta de Machine Learning real.
a cantidad de datos que se generan actualmente en las empresas se está incrementado de forma exponencial. Extraer información valiosa de ellos supone una ventaja competitiva que no se puede menospreciar. En CleverData pensamos que es una oportunidad a la que se le debe prestar especial atención. La gran ventaja es que actualmente no hace falta ser un gurú de los datos para poder aprovechar este tipo de tecnologías. Existen en el mercado herramientas de uso muy sencillo (incluso para profanos en análisis de datos) y asequibles económicamente para cualquier tamaño de empresa que permiten hacer predicciones como las descritas en el apartado anterior. Si quieres ver cómo se genera un modelo como el anterior, revisa este artículo en el que describimos el proceso paso a paso.

Deep Learning.

Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de clase aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas compuestas de transformaciones no lineales múltiples. 1?

El aprendizaje profundo es parte de un conjunto más amplio de métodos de aprendizaje automático basados en asimilar representaciones de datos. Una observación (por ejemplo, una imagen) puede ser representada en muchas formas (por ejemplo, un vector de píxeles), pero algunas representaciones hacen más fácil aprender tareas de interés (por ejemplo, "¿es esta imagen una cara humana?") sobre la base de ejemplos, y la investigación en este área intenta definir qué representaciones son mejores y cómo crear modelos para reconocer estas representaciones.

Varias arquitecturas de aprendizaje profundo, como redes neuronales profundas, redes neuronales profundas convolucionales, y redes de creencia profundas, han sido aplicadas a campos como visión por computador, reconocimiento automático del habla, y reconocimiento de señales de audio y música, y han mostrado producir resultados de vanguardia en varias tareas.

Las GPU para procesamiento general
Conocidas como GPGPU, las tarjetas de vídeo para procesamiento general permiten ejecutar los algoritmos de entrenamiento y evaluación de los modelos de aprendizaje profundo. Debido a la naturaleza altamente paralelizable de estos problemas, la utilización de las GPGPU permite un aumento en el desempeño de varios órdenes de magnitud.

Las GPU en la nube
Los grandes proveedores de servicios en la nube han comenzado a ofrecer servicios de infraestructura especializados para procesamiento con GPU. Nvidia se ha asociado con diversos proveedores para ofrecer dichos servicios, Amazon, Azure e IBM por nombrar algunos.3?

Google Cloud y TensorFlow
Acompañando a su plataforma TensorFlow, Google introdujo la Machine Learning Platform, que provee servicios de aprendizaje automático modernos con modelos preentrenados y un servicio para generar modelos personalizables. A diferencia de los otros proveedores, se presenta como una alternativa PaaS más que un IaaS.

Red Neuronal.

Las redes neuronales (también conocidas como sistemas conexionistas) son un modelo computacional basado en un gran conjunto de unidades neuronales simples (neuronas artificiales) de forma aproximadamente análoga al comportamiento observado en los axones de las neuronas en los cerebros biológicos1?. La información de entrada atraviesa la red neuronal (donde se somete a diversas operaciones) produciendo unos valores de salida.

Criptomonedas.

Una criptomoneda, criptodivisa (del inglés cryptocurrency) o criptoactivo es un medio digital de intercambio que utiliza criptografía fuerte para asegurar las transacciones financieras, controlar la creación de unidades adicionales y verificar la transferencia de activos.1?2?3? Las criptomonedas son un tipo de divisa alternativa y de moneda digital. Las criptomonedas tienen un control descentralizado, en contraposición a las monedas centralizadas y a los bancos centrales.

El control descentralizado de cada moneda funciona a través de una base de datos descentralizada, usualmente una cadena de bloques (en inglés blockchain), que sirve como una base de datos de transacciones financieras pública.

La primera criptomoneda que empezó a operar fue el bitcoin en 20094? y, desde entonces, han aparecido muchas otras con diferentes características y protocolos como Litecoin, Ethereum, Ripple, Dogecoin.

Aplicación de la criptomoneda Monero en un teléfono móvil.
Una criptomoneda, criptodivisa (del inglés cryptocurrency) o criptoactivo es un medio digital de intercambio que utiliza criptografía fuerte para asegurar las transacciones financieras, controlar la creación de unidades adicionales y verificar la transferencia de activos.1?2?3? Las criptomonedas son un tipo de divisa alternativa y de moneda digital. Las criptomonedas tienen un control descentralizado, en contraposición a las monedas centralizadas y a los bancos centrales.

El control descentralizado de cada moneda funciona a través de una base de datos descentralizada, usualmente una cadena de bloques (en inglés blockchain), que sirve como una base de datos de transacciones financieras pública.

La primera criptomoneda que empezó a operar fue el bitcoin en 20094? y, desde entonces, han aparecido muchas otras con diferentes características y protocolos como Litecoin, Ethereum, Ripple, Dogecoin.
En los sistemas de criptomonedas, se garantiza la seguridad, integridad y equilibrio de sus estados de cuentas (contabilidad) por medio de un entramado de agentes (transferencia de archivo segmentada o transferencia de archivo multifuente) que se verifican (desconfían) mutuamente llamados mineros, que son, en su mayoría, público en general y protegen activamente la red (el entramado) al mantener una alta tasa de procesamiento de algoritmos, con la finalidad de tener la oportunidad de recibir una pequeña propina, que se reparte de manera aleatoria.

BlockChain

Lo primero es contextualizarlo. Blockchain significa “cadena de bloques”, su propio nombre nos será muy ilustrativo más adelante para comprender cómo funciona. Nació como actor secundario en la revolución del bitcoin, ya que se trata de la tecnología o el sistema de codificación de la información que está por detrás de la moneda virtual y que sustenta toda su estructura. Pronto se vio el potencial que tenía por sí misma y la cantidad de aplicaciones que permite en otras áreas más allá de las transacciones financieras, como la administración pública o el Internet de las cosas.
Blockchain es una tecnología que permite la transferencia de datos digitales con una codificación muy sofisticada y de una manera completamente segura. Sería como el libro de asientos de contabilidad de una empresa en donde se registran todas las entradas y salidas de dinero; en este caso hablamos de un libro de acontecimientos digitales. 
Pero además, contribuye con una tremenda novedad: esta transferencia no requiere de un intermediario centralizado que identifique y certifique la información, sino que está distribuida en múltiples nodos independientes entre sí que la registran y la validan sin necesidad de que haya confianza entre ellos. Una vez introducida, la información no puede ser borrada, solo se podrán añadir nuevos registros, y no será legitimada a menos que la mayoría de ellos se pongan de acuerdo para hacerlo. 
Junto al nivel de seguridad que proporciona este sistema frente a hackeos, encontramos otra enorme ventaja: aunque la red se cayera, con que solo uno de esos ordenadores o nodos no lo hiciera, la información nunca se perdería o el servicio, según el caso del que hablemos, seguiría funcionando. 
Un ejemplo que ilustra la importancia de la red distribuida está en las redes sociales. Con este sistema, blockchain eliminaría la centralización que imponen aplicaciones como Facebook o Twitter a la hora de identificarnos o validar la procedencia de nuestros mensajes, y la integridad de los mismos sería garantizada por la red de nodos.

“Aunque la red se caiga, con que solo uno de esos nodos no lo haga, la información no se perderá”

Cyberseguridad.

La seguridad informática, también conocida como ciberseguridad o seguridad de tecnología de la información, es el área relacionada con la informática y la telemática que se enfoca en la protección de la infraestructura computacional y todo lo relacionado con esta y, especialmente, la información contenida en una computadora o circulante a través de las redes de computadoras.1? Para ello existen una serie de estándares, protocolos, métodos, reglas, herramientas y leyes concebidas para minimizar los posibles riesgos a la infraestructura o a la información. La ciberseguridad comprende software (bases de datos, metadatos, archivos), hardware, redes de computadoras y todo lo que la organización valore y signifique un riesgo si esta información confidencial llega a manos de otras personas, convirtiéndose, por ejemplo, en información privilegiada.

La definición de seguridad de la información no debe ser confundida con la de «seguridad informática», ya que esta última solo se encarga de la seguridad en el medio informático, pero la información puede encontrarse en diferentes medios o formas, y no solo en medios informáticos.

La seguridad informática es la disciplina que se encarga de diseñar las normas, procedimientos, métodos y técnicas destinados a conseguir un sistema de información seguro y confiable.

Puesto simple, la seguridad en un ambiente de red es la habilidad de identificar y eliminar vulnerabilidades. Una definición general de seguridad debe también poner atención a la necesidad de salvaguardar la ventaja organizacional, incluyendo información y equipos físicos, tales como los mismos computadores. Nadie a cargo de seguridad debe determinar quién y cuándo puede tomar acciones apropiadas sobre un ítem en específico. Cuando se trata de la seguridad de una compañía, lo que es apropiado varía de organización en organización. Independientemente, cualquier compañía con una red debe tener una política de seguridad que se dirija a la conveniencia y la coordinación.

Realidad Virtual.

La realidad virtual (RV) es un entorno de escenas u objetos de apariencia real. La acepción más común refiere a un entorno generado mediante tecnología informática, que crea en el usuario la sensación de estar inmerso en él. Dicho entorno es contemplado por el usuario a través de un dispositivo conocido como gafas o casco de realidad virtual. Este puede ir acompañado de otros dispositivos, como guantes o trajes especiales, que permiten una mayor interacción con el entorno así como la percepción de diferentes estímulos que intensifican la sensación de realidad.
El término realidad virtual (RV) se popularizó a finales de la década de 1980 por Jaron Lanier, uno de los pioneros del campo. Al mismo tiempo, también apareció el término Realidad Artificial (RA). 1? En 1982 el término ciberespacio fue acuñado en una novela por W. Gibson ("Burning Chrome"). La Enciclopedia Británica describe la realidad virtual como "el uso del modelado y la simulación por computadora que permite a una persona interactuar con un entorno sensorial tridimensional (3D) artificial u otro entorno sensorial". 
Además, establece que "las aplicaciones de realidad virtual sumergen al usuario en un entorno generado por computadora que simula la realidad mediante el uso de dispositivos interactivos, que envían y reciben información y se usan como gafas, auriculares, guantes o trajes para el cuerpo". 3? Por ejemplo, un usuario que usa una pantalla montada en la cabeza con un sistema de proyección estereoscópica puede ver imágenes animadas de un entorno virtual. Un término importante es presencia o telepresencia, que se puede describir como una ilusión de "estar allí"
En uso general, la presencia se define como "el hecho o condición de estar presente; el estado de estar con o en el mismo lugar que una persona o cosa; asistencia, compañía, sociedad o asociación ", 5? aunque la presencia también tiene significados diferentes. A principios de la década de 1990, el término presencia se usaba cada vez más para describir la experiencia subjetiva de los participantes en un entorno virtual. Una definición que se usa con mayor frecuencia para entornos virtualmente generados es la de "estar en un lugar o entorno, incluso cuando uno se encuentra físicamente en otro".6? o, más brevemente, "estar allí".

Realidad Aumentada

La realidad aumentada (RA) es el término que se usa para describir al conjunto de tecnologías que permiten que un usuario visualice parte de mundo real a través de un dispositivo tecnológico con información gráfica añadida por éste dispositivo. Este dispositivo o conjunto de dispositivos, añaden información virtual a la información física ya existente; es decir, una parte sintética virtual a la real. De esta manera los elementos físicos tangibles se combinan con elementos virtuales creando así una realidad aumentada en tiempo real.
Además, Paul Milgram y Fumio Kishino (1994) definen la realidad de Milgram-Virtuality Continuum como un continuo que abarca desde el entorno real a un entorno virtual puro. En el medio hay realidad aumentada (está más cerca del entorno real) y virtualidad aumentada (está más cerca del entorno virtual).

La realidad aumentada también supone la incorporación de datos e información digital en un entorno real, por medio del reconocimiento de patrones que se realiza mediante un software. En otras palabras, es una herramienta interactiva que está dando sus primeros pasos alrededor del mundo y que en unos años se verá en todas partes, corriendo y avanzando, sorprendiendo y alcanzando todas las disciplinas: videojuegos, medios masivos de comunicación, arquitectura, educación e incluso en la medicina. Llevará un mundo digital inimaginable al entorno real.
Las tendencias actuales en educación dirigidas a que el alumno sea el propio artífice de su aprendizaje, desempeñando un papel claramente activo en los procesos formativos, y el auge de la Web 2.0, ha posibilitado que los alumnos pasen de ser meros receptores del conocimiento a convertirse en productores de mensajes mediados; es decir, dejan de ser únicamente consumidores de información y adquieren el papel de proconsumidores. Lo significativo de las experiencias educativas en RA, es que ofrecen al alumnado la posibilidad de aprender mediante la construcción de los mensajes audiovisuales y multimedia, así como mediante las pautas que deben seguir para dicha construcción: documentación, elaboración del guion técnico y literario, dominio de la tecnología y concreción del mensaje en el lenguaje de la tecnología elegida. Tanto es así que para poder emprender estas experiencias de RA en educación, los estudiantes deben aprender tanto las utilidades de la tecnología como el lenguaje audiovisual y telemático, para, con ambos, analizar y representar la realidad.

Realidad Mixta.

Un ejemplo de realidad mixta: caracteres virtuales mezclados dentro de una transmisión de video del mundo real.
La realidad mixta (RM), también llamada a veces realidad híbrida, es la combinación de realidad virtual y realidad aumentada. Esta combinación permite crear nuevos espacios en los que interactúan tanto objetos y/o personas reales como virtuales. Es decir, se puede considerar como una mezcla entre la realidad, realidad aumentada, virtualidad aumentada y realidad virtual.

El término realidad mixta no debe confundirse con el de realidad aumentada o RA. La realidad aumentada genera los estímulos a tiempo real para la interacción del usuario, los cuales se superponen sobre el entorno físico de este, mientras que la realidad mixta no sólo permite la interacción del usuario con el entorno virtual sino que también permite que objetos físicos del entorno inmediato del usuario sirvan como elementos de interacción con el entorno virtual.

https://haironon.github.io/Seminario-de-Tecnologia-Aplicada/
